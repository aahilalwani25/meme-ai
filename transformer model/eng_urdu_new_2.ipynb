{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5635226c-8af8-4f65-b5f7-306d8bfd1925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\AAHIL ALWANI\\Desktop\\FYP\\Web App\\Meme_AI\\meme\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "from string import digits\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, Input, Dense,Embedding\n",
    "from keras.models import Model,load_model\n",
    "from keras.utils import plot_model\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import model_from_json\n",
    "import pickle as pkl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3b4f600-56cf-4187-8796-9269e945cea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'urdu_english.csv', encoding='utf-8')\n",
    "df = df[:5000] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8371d937-2e00-4aeb-bda5-54514bf12b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_text = df['english_sentence'].values\n",
    "urdu_text = df['urdu_sentence'].values\n",
    "\n",
    "english_text[0], urdu_text[0]\n",
    "\n",
    "#lowercasing the setences\n",
    "english_text_ = [x.lower() for x in english_text]\n",
    "urdu_text_ = [x.lower() for x in urdu_text]\n",
    "\n",
    "# Text preprocessing\n",
    "english_text_ = [re.sub(\"'\",'',x) for x in english_text_]\n",
    "urdu_text_ = [re.sub(\"'\",'',x) for x in urdu_text_]\n",
    "\n",
    "# remove puntuation\n",
    "def remove_punc(text_list):\n",
    "  table = str.maketrans('', '', string.punctuation)\n",
    "  removed_punc_text = []\n",
    "  for sent in text_list:\n",
    "    sentance = [w.translate(table) for w in sent.split(' ')]\n",
    "    removed_punc_text.append(' '.join(sentance))\n",
    "  return removed_punc_text\n",
    "english_text_ = remove_punc(english_text_)\n",
    "urdu_text_ = remove_punc(urdu_text_)\n",
    "\n",
    "# removing the digits from english sentances\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "removed_digits_text = []\n",
    "for sent in english_text_:\n",
    "  sentance = [w.translate(remove_digits) for w in sent.split(' ')]\n",
    "  removed_digits_text.append(' '.join(sentance))\n",
    "english_text_ = removed_digits_text\n",
    "\n",
    "# removing the digits from the urdu sentances\n",
    "# urdu_text_ = [re.sub(\"[२३०८१५७९४६]\",\"\",x) for x in urdu_text_]\n",
    "urdu_text_ = [re.sub(\"[\\u200d]\",\"\",x) for x in urdu_text_]\n",
    "\n",
    "# removing the stating and ending whitespaces\n",
    "english_text_ = [x.strip() for x in english_text_]\n",
    "urdu_text_ = [x.strip() for x in urdu_text_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e4f4c87-ac13-47ee-9acf-ad5c3ff19b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "urdu_train, urdu_test, eng_train, eng_test = train_test_split(df['urdu_sentence'],df['english_sentence'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1100d01a-8f88-43c7-a9c5-f4ff17f6e7a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10759545-bb7e-4ecd-a879-92bd4259a237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_batch(X= urdu_train,Y=eng_train, batch_size=128):\n",
    "  while True:\n",
    "    for j in range(0, len(X), batch_size):\n",
    "      encoder_data_input = np.zeros((batch_size,max_length_english),dtype='float32') #metrix of batch_size*max_length_english\n",
    "      decoder_data_input = np.zeros((batch_size,max_lenght_urdu),dtype='float32') #metrix of batch_size*max_length_urdu\n",
    "      decoder_target_input = np.zeros((batch_size,max_lenght_urdu,vocab_size_target),dtype='float32') # 3d array one hot encoder decoder target data\n",
    "      for i, (input_text,target_text) in enumerate(zip(X[j:j+batch_size],Y[j:j+batch_size])):\n",
    "        for t, word in enumerate(input_text.split()):\n",
    "          encoder_data_input[i,t] = tokenizer_input.word_index[word] # Here we are storing the encoder \n",
    "                                                                     #seq in row here padding is done automaticaly as \n",
    "                                                                     #we have defined col as max_lenght\n",
    "        for t, word in enumerate(target_text.split()):\n",
    "          # if word == 'START_':\n",
    "          #   word = 'start'\n",
    "          # elif word == 'END_':\n",
    "          #   word = 'end'\n",
    "          decoder_data_input[i,t] = tokenizer_target.word_index[word] # same for the decoder sequence\n",
    "          if t>0:\n",
    "            decoder_target_input[i,t-1,tokenizer_target.word_index[word]] = 1 #target is one timestep ahead of decoder input because it does not have 'start tag'\n",
    "      # print(encoder_data_input.shape())\n",
    "      yield ([encoder_data_input,decoder_data_input],decoder_target_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4254070f-4e5d-44f8-ad38-05e075491627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\AAHIL ALWANI\\Desktop\\FYP\\Web App\\Meme_AI\\meme\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vocab_size_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Define an input sequence and process it.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m encoder_inputs \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m,),name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m emb_layer_encoder \u001b[38;5;241m=\u001b[39m Embedding(\u001b[43mvocab_size_input\u001b[49m,latent_dim, mask_zero\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(encoder_inputs)\n\u001b[0;32m      5\u001b[0m encoder \u001b[38;5;241m=\u001b[39m LSTM(latent_dim, return_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m encoder_outputs, state_h, state_c \u001b[38;5;241m=\u001b[39m encoder(emb_layer_encoder)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab_size_input' is not defined"
     ]
    }
   ],
   "source": [
    "latent_dim = 256\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None,),name=\"encoder_inputs\")\n",
    "emb_layer_encoder = Embedding(vocab_size_input,latent_dim, mask_zero=True)(encoder_inputs)\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(emb_layer_encoder)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,),name=\"decoder_inputs\")\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "emb_layer_decoder = Embedding(vocab_size_target,latent_dim, mask_zero=True)(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(emb_layer_decoder, initial_state=encoder_states)\n",
    "decoder_dense = Dense(vocab_size_target, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "train_samples = len(urdu_train)\n",
    "val_samples = len(urdu_test)\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "\n",
    "model.fit_generator(generator = generator_batch(urdu_train, eng_train, batch_size = batch_size),\n",
    "                    steps_per_epoch = train_samples//batch_size,\n",
    "                    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69523ba-3b9d-4787-ab75-046c318a1649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meme",
   "language": "python",
   "name": "meme"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
